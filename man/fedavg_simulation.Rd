% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/fedavg_simulation.R
\name{fedavg_simulation}
\alias{fedavg_simulation}
\title{Generic FedAvg Simulation}
\usage{
fedavg_simulation(
  client_datasets,
  model_generator,
  evaluation_fn,
  rounds = 10,
  C = 0.1,
  E = 1,
  batch_size = 32,
  optimizer_generator = function(lr) function(p) torch::optim_sgd(p, lr = lr),
  lr_scheduler = function(r) 0.1,
  seed = 123,
  device = "cpu",
  log_file = NULL,
  save_model = NULL
)
}
\arguments{
\item{client_datasets}{A list of torch::dataset objects (one per client).}

\item{model_generator}{A function that returns a fresh instance of the model (torch::nn_module).}

\item{evaluation_fn}{A function(model, device) that returns a named list of metrics (e.g., list(accuracy=0.9)).}

\item{rounds}{Number of communication rounds.}

\item{C}{Fraction of clients to select per round.}

\item{E}{Number of local epochs.}

\item{batch_size}{Local batch size (Inf for full-batch).}

\item{optimizer_generator}{A function(lr) that returns an optimizer_fn for client_train_generic.}

\item{lr_scheduler}{A function(round) that returns the learning rate for that round.}

\item{seed}{Random seed.}

\item{device}{Device to use ("cpu" or "cuda").}

\item{log_file}{Optional path to CSV file for incremental logging of metrics.}

\item{save_model}{Optional path to save the final trained model (e.g., "model.pt"). If NULL, model is not saved.}
}
\value{
A list containing \code{history} (data.frame), \code{final_params}, and \code{final_model} (if save_model is specified).
}
\description{
Runs a Federated Averaging simulation on a generic dataset.
}
\examples{
library(torch)
library(fedavgR)

# 1. Define your model generator
model_gen <- function() {
    nn_sequential(
        nn_linear(10, 20),
        nn_relu(),
        nn_linear(20, 2)
    )
}

# 2. Prepare client datasets (list of torch datasets)
make_classif_dataset <- function(n, p, margin = 1.0) {
    w <- torch_randn(p)
    x <- torch_randn(n, p)
    score <- x$matmul(w) + 0.5 * torch_randn(n)
    # R torch: class indices must start at 1
    y01 <- (score > margin)$to(dtype = torch_long())
    y <- (y01 + 1L)$to(dtype = torch_long())

    dataset(
        name = "toy_cls",
        initialize = function() {
            self$x <- x
            self$y <- y
        },
        .getitem = function(i) {
            list(self$x[i, ], self$y[i])
        },
        .length = function() self$x$size()[1]
    )()
}
K <- 10L
p <- 10L
clients <- lapply(seq_len(K), function(i) {
    make_classif_dataset(n = sample(80:120, 1), p = p)
})

# 3. Define evaluation function
val_ds <- make_classif_dataset(n = 512, p = p)
evaluation_fn <- function(model, device = if (cuda_is_available()) "cuda" else "cpu") {
    model$eval()
    dl <- dataloader(val_ds, batch_size = 256, shuffle = FALSE)
    correct <- 0
    total <- 0
    coro::loop(for (b in dl) {
        x <- b[[1]]$to(device = device)
        y <- b[[2]]$to(device = device)
        with_no_grad({
            logits <- model(x)
            pred <- torch_argmax(logits, dim = 2)
        })
        correct <- correct + as.numeric((pred == y)$sum()$cpu())
        total <- total + length(y)
    })
    list(acc = correct / total)
}

# 4. Run Simulation
results <- fedavg_simulation(
    client_datasets = clients,
    model_generator = model_gen,
    evaluation_fn = evaluation_fn,
    rounds = 5,
    C = 0.3, # ~30\% clients per round
    E = 1, # 1 local epoch
    batch_size = 32,
    optimizer_generator = function(lr) function(p) optim_sgd(p, lr = lr, momentum = 0),
    lr_scheduler = function(r) 0.05,
    seed = 123,
    device = if (cuda_is_available()) "cuda" else "cpu",
    log_file = NULL
)

print(results$history)
}
\references{
McMahan, B., Moore, E., Ramage, D., Hampson, S., & y Arcas, B. A. (2017).
Communication-Efficient Learning of Deep Networks from Decentralized Data.
\emph{Proceedings of the 20th International Conference on Artificial Intelligence
and Statistics (AISTATS)}.
}
